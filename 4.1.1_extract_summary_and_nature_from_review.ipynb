{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":821,"status":"ok","timestamp":1733020135766,"user":{"displayName":"Kevin Chow","userId":"07642969533374817469"},"user_tz":300},"id":"OiLf2HofSDYO","outputId":"7dee3c7c-df21-45f0-9371-143d506c16ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjnWRNhitXew","executionInfo":{"status":"ok","timestamp":1733020147978,"user_tz":300,"elapsed":4275,"user":{"displayName":"Kevin Chow","userId":"07642969533374817469"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcb88af2-1831-4dc6-855a-aeb64f9af908"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","import re\n","import string\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Code + Data')\n","import tobit\n","from tobit import TobitModel\n","\n","from statsmodels.regression import quantile_regression\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.metrics import ndcg_score\n","from sklearn.preprocessing import StandardScaler\n","\n","import gc\n"]},{"cell_type":"code","source":["yelp_data = pd.read_parquet('/content/drive/MyDrive/Code + Data/yelp_data.parquet')\n","print(yelp_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjewMmZ-m-2U","executionInfo":{"status":"ok","timestamp":1733010524985,"user_tz":300,"elapsed":6770,"user":{"displayName":"Kevin Chow","userId":"07642969533374817469"}},"outputId":"b44e57a4-ac56-4661-9ef2-3052c634104e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1870042, 13)\n"]}]},{"cell_type":"code","source":["# Balance Dataset to have equal helpful and non-helpful reviews\n","print(f\"Original dataset size: {len(yelp_data)}\")\n","print(\"0 helpful reviews:\", len(yelp_data[yelp_data['helpful'] == 0]))\n","minority_count = len(yelp_data[yelp_data['helpful'] > 0])\n","print(f\"helpful > 0 reviews: {minority_count}\")\n","\n","majority_class = yelp_data[yelp_data['helpful'] == 0]\n","minority_class = yelp_data[yelp_data['helpful'] > 0]\n","\n","minority_count = len(minority_class)\n","target_majority_size = int(minority_count)  # Keep 1x as many majority class reviews; ~280k\n","\n","# Downsample the majority class\n","downsampled_majority = majority_class.sample(n=target_majority_size, random_state=42)\n","\n","balanced_data = pd.concat([downsampled_majority, minority_class])\n","\n","# Shuffle\n","balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","yelp_data = balanced_data\n","\n","print(f\"New dataset size: {len(yelp_data)}\")\n","print(\"0 helpful reviews\", len(yelp_data[yelp_data['helpful'] == 0]))\n","minority_count = len(yelp_data[yelp_data['helpful'] > 0])\n","print(f\"helpful > 0 reviews: {minority_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wvPrs64ktdf","executionInfo":{"status":"ok","timestamp":1733010529422,"user_tz":300,"elapsed":4449,"user":{"displayName":"Kevin Chow","userId":"07642969533374817469"}},"outputId":"330cb791-4862-4957-de64-325c176b3675"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original dataset size: 1870042\n","0 helpful reviews: 1147389\n","helpful > 0 reviews: 722652\n","New dataset size: 1445304\n","0 helpful reviews 722652\n","helpful > 0 reviews: 722652\n"]}]},{"cell_type":"code","source":["# Add Word Count\n","def word_count(line):\n","  return len(line.split())\n","yelp_data['num_words'] = yelp_data['text'].apply(lambda x: word_count(x))"],"metadata":{"id":"kr1VQpS_vByo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import category natures\n","cat_classes = pd.read_csv(\"/content/drive/MyDrive/Code + Data/category_nature.csv\")\n","# Convert to dict\n","cat_lookup = dict(zip(cat_classes['Category'], cat_classes['Nature']))"],"metadata":{"id":"qUw0eGbr0bN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert helpfulness to be between 0-100 percent, as oppose to 0-1\n","yelp_data['helpful'] = yelp_data['helpful'].apply(lambda x: x * 100)"],"metadata":{"id":"Q2fR2oZUBwiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert categories col to array\n","yelp_data['categories'] = yelp_data['categories'].apply(lambda x: x.split(', '))"],"metadata":{"id":"OYSExk_d5Cw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = yelp_data.columns.tolist()\n","for c in cols:\n","  print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZiObtr1N7Hf","executionInfo":{"status":"ok","timestamp":1733010543750,"user_tz":300,"elapsed":13,"user":{"displayName":"Kevin Chow","userId":"07642969533374817469"}},"outputId":"e1b12697-91d5-4150-9a48-9de9e286ebe5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["review_id\n","user_id\n","business_id\n","stars_reviewer\n","useful\n","text\n","name\n","postal_code\n","stars_business\n","categories\n","total_reviews_for_business\n","helpful\n","num_sentences\n","num_words\n"]}]},{"cell_type":"code","source":["yelp_data.to_parquet('/content/drive/MyDrive/Code + Data/business_nature_yelp_data.parquet')"],"metadata":{"id":"jbeBeQZgfIil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create train, val, test with 60/20/20 split\n","train_val, test = train_test_split(yelp_data, test_size=0.2, random_state=42)\n","train, val = train_test_split(train_val, test_size=0.25, random_state=42)"],"metadata":{"id":"gBiV3XEYmKmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hand labeling + taking majority class as the business nature"],"metadata":{"id":"b0mJDCwO8fVM"}},{"cell_type":"code","source":["# Initial code for hand labeling categories\n","\n","# all_categories = yelp_data['categories'].str.split(', ').explode()\n","# unique_categories = all_categories.unique()\n","# print(len(unique_categories))\n","# # Save to df for export to an Excel file for hand labeling\n","# nature_df = pd.DataFrame({'Category': unique_categories, 'Nature': ''})\n","\n","# # Save to Excel file\n","# excel_file_path = '/content/drive/MyDrive/Code + Data/category_nature.xlsx'\n","# nature_df.to_excel(excel_file_path, index=False)"],"metadata":{"id":"6pCVDaKXy3nY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add a col with list of each category's nature (0 or 1)\n","def map_categories_to_nature(categories):\n","  return [cat_lookup[cat] for cat in categories]\n","\n","train_weights_processed['categories_nature'] = train_weights_processed['categories'].apply(map_categories_to_nature)\n","val_weights_processed['categories_nature'] = val_weights_processed['categories'].apply(map_categories_to_nature)\n","test_weights_processed['categories_nature'] = test_weights_processed['categories'].apply(map_categories_to_nature)"],"metadata":{"id":"_I_Oyby05KrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get majority nature based on 'categories_nature' col - yielded better results than trying to utilize word embeddings\n","def get_majority_nature(nature_list):\n","  search_count = 0\n","  experience_count = 0\n","  for n in nature_list:\n","    if n == 0:\n","      search_count += 1\n","    elif n == 1:\n","      experience_count += 1\n","\n","  if search_count > experience_count:\n","    return 0\n","  else:\n","    return 1 # For equal counts of each, assume experience due to skew of businesses\n","\n","train_weights_processed['nature'] = train_weights_processed['categories_nature'].apply(get_majority_nature)\n","val_weights_processed['nature'] = val_weights_processed['categories_nature'].apply(get_majority_nature)\n","test_weights_processed['nature'] = test_weights_processed['categories_nature'].apply(get_majority_nature)"],"metadata":{"id":"M--NOvXs5VFs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculating Subjective and Objective Weights based on Similarity b/w Extractive Summary (TF-IDF) and Business Categories\n"],"metadata":{"id":"dl0SQ02ahRPz"}},{"cell_type":"code","source":["def pre_process_review(review_text):\n","  review_text = review_text.lower()\n","  stop_words = set(stopwords.words('english'))\n","  review_text = ' '.join([word for word in review_text.split() if word not in stop_words])\n","  return review_text"],"metadata":{"id":"beh8hta7mct2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['pp_text'] = train['text'].apply(lambda x: pre_process_review(x))\n","val['pp_text'] = val['text'].apply(lambda x: pre_process_review(x))\n","test['pp_text'] = test['text'].apply(lambda x: pre_process_review(x))"],"metadata":{"id":"lwHWRoPYmtaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_pp.parquet') # Save to parquet file as checkpoint\n","val.to_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_pp.parquet') # Save to parquet file as checkpoint\n","test.to_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_pp.parquet') # Save to parquet file as checkpoint"],"metadata":{"id":"SB7COx3zn6so"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extractive summarization w/ TF-IDF\n","def preprocess_and_vectorize(text):\n","    sentences = sent_tokenize(text)  # Tokenize into sentences\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform(sentences)\n","    return sentences, tfidf_matrix, vectorizer\n","def score_sentences(tfidf_matrix):\n","    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n","    return sentence_scores\n","def generate_summary(text, num_sentences=3):\n","    sentences, tfidf_matrix, vectorizer = preprocess_and_vectorize(text)\n","    scores = score_sentences(tfidf_matrix)\n","    ranked_sentences = sorted(((score, index) for index, score in enumerate(scores)), reverse=True)\n","    top_sentences = sorted(ranked_sentences[:num_sentences], key=lambda x: x[1])\n","    summary = \" \".join([sentences[index] for _, index in top_sentences])\n","    return summary"],"metadata":{"id":"FdcwfmU72Z9L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['extractive_summary'] = train['pp_text'].apply(lambda x: generate_summary(x))\n","val['extractive_summary'] = val['pp_text'].apply(lambda x: generate_summary(x))\n","test['extractive_summary'] = test['pp_text'].apply(lambda x: generate_summary(x))"],"metadata":{"id":"Pf9ghFAVTq3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_pp_summarized.parquet')\n","val.to_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_pp_summarized.parquet')\n","test.to_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_pp_summarized.parquet')"],"metadata":{"id":"f0XkKn81r8de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_similarity(processed_text, keywords):\n","    # Create a CountVectorizer object\n","    vectorizer = CountVectorizer().fit([processed_text] + keywords)\n","\n","    # Convert text and keywords to vectors\n","    text_vector = vectorizer.transform([processed_text])\n","    keyword_vectors = vectorizer.transform(keywords)\n","\n","    # Calculate cosine similarity\n","    similarities = cosine_similarity(text_vector, keyword_vectors)\n","\n","    # Create a dictionary of keyword-similarity pairs\n","    similarity_scores = {keyword: score for keyword, score in zip(keywords, similarities[0])}\n","\n","    return similarity_scores\n","\n","# Test usage\n","# text = subset_rev['summary'].iloc[0]\n","# keywords = subset_rev['categories'].iloc[0]\n","\n","# similarity_results = calculate_similarity(text, keywords)\n","# print(similarity_results)"],"metadata":{"id":"tJ1mJpzb3g-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['similarity_results'] = train.apply(lambda row: calculate_similarity(row['extractive_summary'], row['categories']), axis=1)\n","val['similarity_results'] = val.apply(lambda row: calculate_similarity(row['extractive_summary'], row['categories']), axis=1)\n","test['similarity_results'] = test.apply(lambda row: calculate_similarity(row['extractive_summary'], row['categories']), axis=1)"],"metadata":{"id":"ztJODhJosfHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_pp_summarized_similarity.parquet')\n","val.to_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_pp_summarized_similarity.parquet')\n","test.to_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_pp_summarized_similarity.parquet')"],"metadata":{"id":"6z6NEtG6vaDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_weights_by_category(similarity_results):\n","    category_values = {\"0\": [], \"1\": []}\n","    for key, value in similarity_results.items():\n","        category = cat_lookup.get(key)\n","        category_values[str(category)].append(value)\n","    weights = {}\n","    for category, values in category_values.items():\n","        if values:\n","            weights[category] = sum(values) / len(values)  # Average value\n","        else:\n","            weights[category] = 0.0  # Default to 0 if no values\n","    return weights\n"],"metadata":{"id":"gwT23Ji87P_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the function to each entry in 'summary' column\n","train['weights'] = train['similarity_results'].apply(lambda x: calculate_weights_by_category(x))\n","val['weights'] = val['similarity_results'].apply(lambda x: calculate_weights_by_category(x))\n","test['weights'] = test['similarity_results'].apply(lambda x: calculate_weights_by_category(x))"],"metadata":{"id":"ilnbFlHAvwHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_pp_summarized_similarity_weights.parquet')\n","val.to_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_pp_summarized_similarity_weights.parquet')\n","test.to_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_pp_summarized_similarity_weights.parquet')"],"metadata":{"id":"27_SJOl-wftp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final df processing for train/val/test"],"metadata":{"id":"Bc7KNkNBA5nY"}},{"cell_type":"code","source":["# Load processed data\n","train_weights_processed = pd.read_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_pp_summarized_similarity_weights.parquet')\n","val_weights_processed = pd.read_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_pp_summarized_similarity_weights.parquet')\n","test_weights_processed = pd.read_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_pp_summarized_similarity_weights.parquet')"],"metadata":{"id":"9AHIRteQA4xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract weights for search and experience nature\n","train_weights_processed['search_similarity'] = train_weights_processed['weights'].apply(lambda x: x[\"0\"])\n","train_weights_processed['experience_similarity'] = train_weights_processed['weights'].apply(lambda x: x[\"1\"])\n","\n","val_weights_processed['search_similarity'] = val_weights_processed['weights'].apply(lambda x: x[\"0\"])\n","val_weights_processed['experience_similarity'] = val_weights_processed['weights'].apply(lambda x: x[\"1\"])\n","\n","test_weights_processed['search_similarity'] = test_weights_processed['weights'].apply(lambda x: x[\"0\"])\n","test_weights_processed['experience_similarity'] = test_weights_processed['weights'].apply(lambda x: x[\"1\"])"],"metadata":{"id":"M51QS38MBUBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_weights_processed.to_parquet('/content/drive/MyDrive/Code + Data/bn_train_data_final.parquet')\n","val_weights_processed.to_parquet('/content/drive/MyDrive/Code + Data/bn_val_data_final.parquet')\n","test_weights_processed.to_parquet('/content/drive/MyDrive/Code + Data/bn_test_data_final.parquet')"],"metadata":{"id":"il6Wn3IFBCiY"},"execution_count":null,"outputs":[]}]}